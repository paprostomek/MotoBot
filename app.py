import streamlit as st
import os
import json
import requests
import re
import chromadb
from sentence_transformers import SentenceTransformer
import google.generativeai as genai
from groq import Groq

# ==========================================
import streamlit as st
import os
import google.generativeai as genai
from groq import Groq

# ==========================================
# KONFIGURACJA AI (HYBRYDA: GROQ + GOOGLE)
# ==========================================
st.sidebar.title("‚öôÔ∏è Konfiguracja AI")

# Funkcja bezpiecznie pobierajƒÖca klucze (z chmury lub zmiennych)
def get_key(name):
    # Najpierw sprawdza st.secrets (chmura), potem zmienne ≈õrodowiskowe
    if name in st.secrets:
        return st.secrets[name]
    return os.environ.get(name)

groq_key = get_key("GROQ_API_KEY")
google_key = get_key("GOOGLE_API_KEY")

# Wyb√≥r silnika (automatyczny lub rƒôczny)
engine = "Brak"
if groq_key or google_key:
    # Je≈õli mamy oba klucze, dajemy wyb√≥r. Je≈õli jeden - ustawiamy go automatycznie.
    dostepne_opcje = []
    if groq_key: dostepne_opcje.append("Groq (Llama 3 - Szybki)")
    if google_key: dostepne_opcje.append("Google (Gemini - Dok≈Çadny)")
    
    engine = st.sidebar.radio("Wybierz silnik AI:", dostepne_opcje)
else:
    st.error("‚ùå Brak kluczy API! Skonfiguruj Secrets w Streamlit Cloud.")
    st.stop() # Zatrzymuje aplikacjƒô, ≈ºeby nie wywali≈Ça b≈Çƒôdu dalej

# --- UNIWERSALNA FUNKCJA GENEROWANIA ---
def generate_ai_response(prompt_text):
    # 1. ≈öcie≈ºka GROQ
    if "Groq" in engine:
        try:
            client = Groq(api_key=groq_key)
            completion = client.chat.completions.create(
                model="llama3-70b-8192", 
                messages=[{"role": "user", "content": prompt_text}],
                temperature=0.6,
                max_tokens=1000
            )
            return completion.choices[0].message.content
        except Exception as e:
            return f"‚ö†Ô∏è B≈ÇƒÖd Groq: {e}"

    # 2. ≈öcie≈ºka GOOGLE
    elif "Google" in engine:
        try:
            genai.configure(api_key=google_key)
            # Pr√≥ba u≈ºycia modelu 2.5 flash, potem fallback na 2.5 flash lite
            models = ['gemini-2.5-flash', 'gemini-2.5-flash-lite']
            active_model = None
            for m in models:
                try:
                    active_model = genai.GenerativeModel(m)
                    break
                except: continue
            
            if active_model:
                return active_model.generate_content(prompt_text).text
            else:
                return "Nie uda≈Ço siƒô po≈ÇƒÖczyƒá z ≈ºadnym modelem Google."
        except Exception as e:
            return f"‚ö†Ô∏è B≈ÇƒÖd Google: {e}"

# ==========================================
# Funkcja do sprawdzania VIN
# ==========================================
def get_car_from_vin(vin: str):
    vin = vin.strip().upper()
    
    # --- 1. MOCK (≈öCIƒÑGA NA ZALICZENIE) ---
    # Tutaj wpisujemy VIN-y, kt√≥re majƒÖ dzia≈Çaƒá na 100% podczas prezentacji
    if vin == "WBA1R51050V764951":  # Tw√≥j VIN z BMW
        return "BMW Seria 1 (E87) 2004-2011"
    
    if vin == "VWZZZ1JZEW000001":   # Przyk≈Çadowy VIN Golfa IV
        return "Volkswagen Golf IV 1.9 TDI"

    # --- 2. STANDARDOWE SPRAWDZANIE (API USA) ---
    if not re.match(r"^[A-HJ-NPR-Z0-9]{17}$", vin):
        return None
    try:
        url = f"https://vpic.nhtsa.dot.gov/api/vehicles/decodevin/{vin}?format=json"
        r = requests.get(url, timeout=5)
        if r.status_code == 200:
            data = r.json()["Results"]
            make = next((x["Value"] for x in data if x["Variable"] == "Make"), "")
            model_car = next((x["Value"] for x in data if x["Variable"] == "Model"), "")
            year = next((x["Value"] for x in data if x["Variable"] == "Model Year"), "")
            
            # API czasem zwraca puste pola dla aut z Europy
            if make and model_car:
                return f"{make} {model_car} {year}".strip()
    except:
        pass
    
    return None
# ==========================================
# Wczytanie bazy czƒô≈õci z JSON
# ==========================================
if not os.path.exists("baza_czesci.json"):
    st.error("Nie znaleziono pliku 'baza_czesci.json'!")
    st.stop()

with open("baza_czesci.json", "r", encoding="utf-8") as f:
    data_json = json.load(f)

def prepare_docs(data):
    docs, ids, metadatas = [], [], []
    for i, item in enumerate(data):
        text = (f"Produkt: {item['nazwa']}. Cena: {item['cena']}. "
                f"Opis: {item['opis']}. Pasuje do: {', '.join(item['pasuje_do'])}.")
        docs.append(text)
        ids.append(str(i))
        metadatas.append({"source": "json"})
    return docs, ids, metadatas

docs, ids, metadatas = prepare_docs(data_json)

st.sidebar.info("‚è≥ Tworzƒô bazƒô wektorowƒÖ Chroma...")
embedder = SentenceTransformer('all-MiniLM-L6-v2')
chroma_client = chromadb.Client()

try:
    chroma_client.delete_collection(name="czesci_auto")
except:
    pass

collection = chroma_client.create_collection(name="czesci_auto")
embeddings = embedder.encode(docs).tolist()
collection.add(documents=docs, embeddings=embeddings, metadatas=metadatas, ids=ids)
st.sidebar.success("‚úÖ Baza gotowa!")

# ==========================================
# Funkcja chat bota (ZMODYFIKOWANA - LEPSZY STYL + PAMIƒòƒÜ)
# ==========================================
def ask_bot(user_question, history, vin_context=None):
    try:
        # 1. Szukanie w bazie wektorowej (RAG)
        query_embed = embedder.encode([user_question]).tolist()
        results = collection.query(query_embeddings=query_embed, n_results=3)
        found_text = "\n".join(results['documents'][0])

        # 2. Formatowanie historii rozmowy do tekstu
        history_text = ""
        for msg in history:
            role = "KLIENT" if msg["role"] == "user" else "SPRZEDAWCA"
            history_text += f"{role}: {msg['content']}\n"

        # 3. Prompt (Instrukcja dla AI) - TUTAJ JEST ZMIANA
        prompt = f"""
Jeste≈õ profesjonalnym, uprzejmym i pomocnym ekspertem w sklepie motoryzacyjnym.
Twoim celem jest doradziƒá klientowi najlepszy produkt i sprawiƒá, by czu≈Ç siƒô dobrze obs≈Çu≈ºony.

ZASADY ODPOWIEDZI:
1. BƒÖd≈∫ komunikatywny i u≈ºywaj pe≈Çnych zda≈Ñ (np. "Do Twojego Golfa polecam...", "Mamy ≈õwietny olej...").
2. Je≈õli klient pyta og√≥lnie (np. "klocki"), a nie poda≈Ç szczeg√≥≈Ç√≥w (prz√≥d/ty≈Ç), BƒÑD≈π PROAKTYWNY i dopytaj o te szczeg√≥≈Çy.
3. Je≈õli czego≈õ nie ma w bazie, przepro≈õ i zaproponuj co≈õ innego lub zapytaj o inne potrzeby.
4. Korzystaj z HISTORII ROZMOWY, aby wiedzieƒá o czym m√≥wili≈õcie wcze≈õniej (nie pytaj o to samo dwa razy).
5. STOSUJ CROSS-SELLING: Je≈õli klient pyta o olej, zapytaj czy potrzebuje te≈º filtra oleju. Je≈õli o klocki hamulcowe, zapytaj o stan tarcz. BƒÖd≈∫ dobrym sprzedawcƒÖ!
DANE DO TWOJEJ DYSPOZYCJI:
--- BAZA PRODUKT√ìW W SKLEPIE ---
{found_text}

--- AUTO KLIENTA ---
{vin_context if vin_context else "Nieznane (dopytaj o VIN je≈õli to konieczne do doboru czƒô≈õci)"}

--- HISTORIA ROZMOWY ---
{history_text}

--- NOWE PYTANIE KLIENTA ---
{user_question}
"""
       # Nowe wywo≈Çanie (korzysta z naszej funkcji hybrydowej)
        response_text = generate_ai_response(prompt)
        return response_text
    except Exception as e:
        return f"‚ö†Ô∏è WystƒÖpi≈Ç b≈ÇƒÖd: {e}"

# ==========================================
# STREAMLIT UI
# ==========================================
st.title("üöó MotoBot AI")

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Dzie≈Ñ dobry! Jestem Twoim wirtualnym doradcƒÖ. Podaj numer VIN lub powiedz, jakiej czƒô≈õci szukasz?"}]

if "current_car" not in st.session_state:
    st.session_state.current_car = None

with st.sidebar:
    st.title("üîß Status pojazdu")
    if st.session_state.current_car:
        st.success(st.session_state.current_car)
        if st.button("Zresetuj pojazd"):
            st.session_state.current_car = None
            st.rerun()
    else:
        st.info("Brak zidentyfikowanego pojazdu")

# Wy≈õwietlanie historii czatu
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# Pobieranie wej≈õcia od u≈ºytkownika
if prompt := st.chat_input("Wpisz VIN lub pytanie..."):
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    detected_car = get_car_from_vin(prompt)

    if detected_car:
        st.session_state.current_car = detected_car
        answer = f"‚úÖ ≈öwietnie! Zidentyfikowa≈Çem Tw√≥j pojazd: **{detected_car}**. Teraz mogƒô precyzyjnie dobraƒá czƒô≈õci. Czego potrzebujesz?"
    else:
        query = prompt
        if st.session_state.current_car:
            query += f" Kontekst pojazdu: {st.session_state.current_car}."

        with st.spinner("Przeszukujƒô magazyn..."):
            answer = ask_bot(query, st.session_state.messages, st.session_state.current_car)

    with st.chat_message("assistant"):
        st.markdown(answer)


    st.session_state.messages.append({"role": "assistant", "content": answer})
